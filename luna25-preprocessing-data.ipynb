{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def standardize_nodule_df(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to standardize CSV files exported from CIRRUS Lung Screening for temporal analysis\n",
    "    Args:\n",
    "        dataset (pandas.DataFrame): input dataset\n",
    "    Returns:\n",
    "        pandas.DataFrame: output dataset\n",
    "    \"\"\"\n",
    "    dataset.PatientID = dataset.PatientID.astype(int)\n",
    "    dataset.LesionID = dataset.LesionID.astype(int)\n",
    "    dataset.StudyDate = dataset.StudyDate.astype(int)\n",
    "    dataset[\"NoduleID\"] = [\n",
    "        f\"{p}_{l}\" for p, l in zip(dataset.PatientID, dataset.LesionID)\n",
    "    ]\n",
    "    dataset[\"AnnotationID\"] = [\n",
    "        f\"{n}_{s}\" for n, s in zip(dataset.NoduleID, dataset.StudyDate)\n",
    "    ]\n",
    "    return dataset\n",
    "\n",
    "def transform(input_image, point):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image: SimpleITK Image\n",
    "    point: array of points\n",
    "    Returns\n",
    "    -------\n",
    "    tNumpyOrigin\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        list(\n",
    "            reversed(\n",
    "                input_image.TransformContinuousIndexToPhysicalPoint(\n",
    "                    list(reversed(point))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def itk_image_to_numpy_image(input_image: sitk.Image) -> Tuple[np.array, Dict]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image: SimpleITK image\n",
    "    Returns\n",
    "    -------\n",
    "    numpyImage: SimpleITK image to numpy image\n",
    "    header: dict containing origin, spacing and transform in numpy format\n",
    "    \"\"\"\n",
    "\n",
    "    numpyImage = sitk.GetArrayFromImage(input_image)\n",
    "    numpyOrigin = np.array(list(reversed(input_image.GetOrigin())))\n",
    "    numpySpacing = np.array(list(reversed(input_image.GetSpacing())))\n",
    "\n",
    "    # get numpyTransform\n",
    "    tNumpyOrigin = transform(input_image, np.zeros((numpyImage.ndim,)))\n",
    "    tNumpyMatrixComponents = [None] * numpyImage.ndim\n",
    "    for i in range(numpyImage.ndim):\n",
    "        v = [0] * numpyImage.ndim\n",
    "        v[i] = 1\n",
    "        tNumpyMatrixComponents[i] = transform(input_image, v) - tNumpyOrigin\n",
    "    numpyTransform = np.vstack(tNumpyMatrixComponents).dot(np.diag(1 / numpySpacing))\n",
    "\n",
    "    # define necessary image metadata in header\n",
    "    header = {\n",
    "        \"origin\": numpyOrigin,\n",
    "        \"spacing\": numpySpacing,\n",
    "        \"transform\": numpyTransform,\n",
    "    }\n",
    "\n",
    "    return numpyImage, header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract nodules (convert from CT to .nii.gz files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import SimpleITK as sitk\n",
    "import multiprocessing\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(levelname)s][%(asctime)s] %(message)s\",\n",
    "    datefmt=\"%I:%M:%S\",\n",
    ")\n",
    "\n",
    "MUST_CONTAIN = [\n",
    "    \"SeriesInstanceUID\",\n",
    "    \"StudyDate\",\n",
    "    \"CoordX\",\n",
    "    \"CoordY\",\n",
    "    \"CoordZ\",\n",
    "    \"PatientID\",\n",
    "]\n",
    "\n",
    "SAVE_FORMATS = [\n",
    "    \".nii.gz\",\n",
    "    \".nii\",\n",
    "    \".mha\",\n",
    "    \".mhd\",\n",
    "]\n",
    "\n",
    "\n",
    "class NoduleExtractor:\n",
    "    \"\"\"\n",
    "    Class to load a dataset and extract nodule patches\n",
    "    and save them to disk in a format compatible with SimpleITK\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: Path,\n",
    "        image_root: Path,\n",
    "        output_path: Path,\n",
    "        postfix: str = \"\",\n",
    "        patch_size: np.array = np.array([128, 128, 64]),\n",
    "        save_format: str = \".nii.gz\",\n",
    "    ) -> None:\n",
    "\n",
    "        self.dataset: pandas.DataFrame = pandas.read_csv(csv_path)\n",
    "        self.output_path = output_path\n",
    "        self.image_root = image_root\n",
    "        self.postfix = postfix\n",
    "        self.patch_size = np.array(patch_size)\n",
    "        self.save_format = save_format\n",
    "\n",
    "        self.output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        assert save_format in SAVE_FORMATS, \"save format not supported\"\n",
    "\n",
    "        assert np.all(\n",
    "            [k in self.dataset.keys() for k in MUST_CONTAIN]\n",
    "        ), f\"keys missing: the CSV must contain {MUST_CONTAIN}\"\n",
    "\n",
    "        self.dataset = standardize_nodule_df(self.dataset)\n",
    "\n",
    "    def process_seriesuid(\n",
    "        self,\n",
    "        seriesuid: str,\n",
    "    ) -> None:\n",
    "        \"\"\"Function to generate 3D patches around nodules for a given SeriesInstanceUID\n",
    "        Expects the dataframe `dataset` to be globally present\n",
    "        Args:\n",
    "            seriesuid (str)\n",
    "        \"\"\"\n",
    "        logging.info(f\"Extracting nodule blocks from {seriesuid}\")\n",
    "\n",
    "        subset = self.dataset[self.dataset.SeriesInstanceUID == seriesuid]\n",
    "        extent = self.patch_size // 2\n",
    "\n",
    "        image_path = str(self.image_root / f\"{seriesuid}.mha\")\n",
    "\n",
    "        # Check if nodule is already extracted\n",
    "        coords = np.array(\n",
    "            [\n",
    "                subset.CoordX,\n",
    "                subset.CoordY,\n",
    "                subset.CoordZ,\n",
    "            ]\n",
    "        ).transpose()\n",
    "\n",
    "        for i, coord in enumerate(coords):\n",
    "\n",
    "            pd = subset.iloc[i]\n",
    "            output_path = (\n",
    "                    self.output_path\n",
    "                    / f\"{pd.NoduleID}_{int(pd.StudyDate)}{self.postfix}{self.save_format}\"\n",
    "            )\n",
    "\n",
    "            if Path(output_path).is_file():\n",
    "                logging.info(f\"{pd.NoduleID} of {seriesuid} is already extracted\")\n",
    "\n",
    "            else:\n",
    "                if Path(image_path).is_file():\n",
    "\n",
    "                    image = sitk.ReadImage(image_path)\n",
    "\n",
    "                    pad = False\n",
    "\n",
    "                    coords = np.array(\n",
    "                        [\n",
    "                            subset.CoordX,\n",
    "                            subset.CoordY,\n",
    "                            subset.CoordZ,\n",
    "                        ]\n",
    "                    ).transpose()\n",
    "\n",
    "                    for coord in coords:\n",
    "\n",
    "                        coord = np.array(image.TransformPhysicalPointToIndex(coord))\n",
    "                        upper_limit_breach = np.any(coord - extent < 0)\n",
    "                        lower_limit_breach = np.any(coord + extent > np.array(image.GetSize()))\n",
    "\n",
    "                        if upper_limit_breach or lower_limit_breach:\n",
    "                            pad = True\n",
    "\n",
    "                    if pad:\n",
    "\n",
    "                        image = sitk.ConstantPad(\n",
    "                            image,\n",
    "                            [int(e) for e in extent],\n",
    "                            [int(e) for e in extent],\n",
    "                            constant=-1024,\n",
    "                        )\n",
    "\n",
    "                    for i, coord in enumerate(coords):\n",
    "\n",
    "                        coord = image.TransformPhysicalPointToIndex(coord)\n",
    "\n",
    "                        pd = subset.iloc[i]\n",
    "\n",
    "                        output_path = (\n",
    "                            self.output_path\n",
    "                            / f\"{pd.NoduleID}_{int(pd.StudyDate)}{self.postfix}{self.save_format}\"\n",
    "                        )\n",
    "\n",
    "                        image_patch = image[\n",
    "                            int(coord[0] - extent[0]) : int(coord[0] + extent[0]),\n",
    "                            int(coord[1] - extent[1]) : int(coord[1] + extent[1]),\n",
    "                            int(coord[2] - extent[2]) : int(coord[2] + extent[2]),\n",
    "                        ]\n",
    "\n",
    "                        if image_patch.GetSize() == tuple(self.patch_size):\n",
    "                            sitk.WriteImage(image_patch, str(output_path), True)\n",
    "                        else:\n",
    "                            logging.info(f\"Incorrect patch size in: {seriesuid}\")\n",
    "\n",
    "                else:\n",
    "                    logging.info(f\"Missing mha file: {str(image_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "extractor = NoduleExtractor(\n",
    "    csv_path=Path(\n",
    "        'data/LUNA25_Public_Training_Development_Data.csv'\n",
    "    ),\n",
    "    image_root=Path('data/raw/'),\n",
    "    output_path=Path(\n",
    "        'data/processed/'\n",
    "    ),\n",
    "    postfix=\"_0000\",\n",
    "    save_format=\".nii.gz\",\n",
    ")\n",
    "\n",
    "pool = multiprocessing.Pool(16)\n",
    "pool.map(\n",
    "    extractor.process_seriesuid,\n",
    "    extractor.dataset.SeriesInstanceUID.unique(),\n",
    ")\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert 2 nodule (.npy files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pandas\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(levelname)s][%(asctime)s] %(message)s\",\n",
    "    datefmt=\"%I:%M:%S\",\n",
    ")\n",
    "\n",
    "class NodulePreProcessor:\n",
    "    \"\"\"\n",
    "    Class to preprocess nodule blocks and store as numpy files\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: Path,\n",
    "        csv_path: Path,\n",
    "        save_path: Path,\n",
    "    ) -> None:\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.dataset = pandas.read_csv(csv_path)\n",
    "        self.dataset = standardize_nodule_df(self.dataset)\n",
    "\n",
    "        self.dst_image_path = self.save_path / \"image\"\n",
    "        self.dst_metadata_path = self.save_path / \"metadata\"\n",
    "\n",
    "        self.dst_image_path.mkdir(exist_ok=True, parents=True)\n",
    "        self.dst_metadata_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "    def prepare_numpy_files(self, annotation_id):\n",
    "        \"\"\"Function to load a nifty file (prepared for nnU-Net)\n",
    "        and convert that into numpy files for fast loading during training\n",
    "        Args:\n",
    "            annotation_id (str): The unique annotation ID for a nodule,\n",
    "            f\"{PatientID}_{LesionID}_{StudyDate}\"\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(f\"processing {annotation_id}\")\n",
    "\n",
    "        image_path = self.data_path / f\"{annotation_id}_0000.nii.gz\"\n",
    "\n",
    "        if image_path.is_file():\n",
    "\n",
    "            image = sitk.ReadImage(str(image_path))\n",
    "            image, header = utils.itk_image_to_numpy_image(image)\n",
    "\n",
    "            np.save(self.dst_image_path / f\"{annotation_id}.npy\", image)\n",
    "            np.save(self.dst_metadata_path / f\"{annotation_id}.npy\", header)\n",
    "\n",
    "        else:\n",
    "            logging.info(f\"{annotation_id} missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preprocessor = NodulePreProcessor(\n",
    "    data_path=Path(args.data_path),\n",
    "    csv_path=Path(args.csv_path),\n",
    "    save_path=Path(args.save_path),\n",
    ")\n",
    "\n",
    "tasks = preprocessor.dataset.AnnotationID.unique()\n",
    "\n",
    "pool = multiprocessing.Pool(args.num_workers)\n",
    "pool.map(preprocessor.prepare_numpy_files, tasks)\n",
    "pool.close()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8633985,
     "sourceId": 13589236,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8634044,
     "sourceId": 13589320,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8634049,
     "sourceId": 13589325,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8668752,
     "sourceId": 13637875,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8669793,
     "sourceId": 13639323,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8669826,
     "sourceId": 13639361,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
